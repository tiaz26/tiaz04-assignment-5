{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN class\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO: Implement the fit method\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: Implement the predict method\n",
    "        predictions = []\n",
    "        \n",
    "        for example in X:\n",
    "            # Calculate distances from the current example to all training points\n",
    "            distances = []\n",
    "            for i in range(len(self.X_train)):\n",
    "                distance = self.compute_distance(self.X_train[i], example)\n",
    "                distances.append((i, distance))\n",
    "\n",
    "            # Sort the distances and get the indices of the k nearest neighbors\n",
    "            distances.sort(key=lambda x: x[1])\n",
    "            k_neighbors = distances[:self.k]\n",
    "\n",
    "            # Retrieve the labels of the k nearest neighbors\n",
    "            k_y_values = [self.y_train[neighbor[0]] for neighbor in k_neighbors]\n",
    "\n",
    "            # Perform majority voting for classification\n",
    "            prediction = max(set(k_y_values), key=k_y_values.count)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def compute_distance(self, X1, X2):\n",
    "        # TODO: Implement distance computation based on self.distance_metric\n",
    "        # Hint: Use numpy operations for efficient computation\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((X1 - X2) ** 2))\n",
    "        if self.distance_metric == 'manhattan':\n",
    "            return np.sum(np.abs(X1 - X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data preprocessing function\n",
    "def preprocess_data(train_path, test_path):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    # TODO: Implement data preprocessing\n",
    "    # Handle categorical variables, scale features, etc.\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    drop_columns = ['id', 'CustomerId', 'Surname']\n",
    "    train_data = train_data.drop(columns=drop_columns)\n",
    "    test_data = test_data.drop(columns=drop_columns)\n",
    "\n",
    "    # Fill missing values for numerical columns with mean\n",
    "    numeric_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "    for feature in numeric_features:\n",
    "        train_data[feature].fillna(train_data[feature].mean(), inplace=True)\n",
    "        test_data[feature].fillna(train_data[feature].mean(), inplace=True)  # Use train mean for test\n",
    "\n",
    "    # One-hot encoding for categorical variables ('Geography' and 'Gender')\n",
    "    train_data = pd.get_dummies(train_data, columns=['Geography', 'Gender'], drop_first=True)\n",
    "    test_data = pd.get_dummies(test_data, columns=['Geography', 'Gender'], drop_first=True)\n",
    "\n",
    "    # Align train and test data columns\n",
    "    test_data = test_data.reindex(columns=train_data.columns.drop('Exited'), fill_value=0)\n",
    "\n",
    "    # Separate features and target labels (for train only)\n",
    "    X_train = train_data.drop('Exited', axis=1)\n",
    "    y_train = train_data['Exited']\n",
    "    X_test = test_data.copy()  # Assuming test doesn't have 'Exited'\n",
    "\n",
    "    # Manual Min-Max Scaling\n",
    "    for feature in numeric_features:\n",
    "        min_value = X_train[feature].min()\n",
    "        max_value = X_train[feature].max()\n",
    "        X_train[feature] = (X_train[feature] - min_value) / (max_value - min_value)\n",
    "        # Scale test data using the same min and max from the train data\n",
    "        X_test[feature] = (X_test[feature] - min_value) / (max_value - min_value)\n",
    "\n",
    "    return X_train.values, y_train.values, X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "# Accuracy calculation\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "# Precision calculation\n",
    "def precision(y_true, y_pred):\n",
    "    true_positive = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    predicted_positive = np.sum(y_pred == 1)\n",
    "    return true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "\n",
    "# Recall calculation\n",
    "def recall(y_true, y_pred):\n",
    "    true_positive = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    actual_positive = np.sum(y_true == 1)\n",
    "    return true_positive / actual_positive if actual_positive > 0 else 0\n",
    "\n",
    "# F1-Score calculation\n",
    "def f1_score(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "# ROC AUC calculation (simplified, assuming binary classification)\n",
    "def roc_auc(y_true, y_scores):\n",
    "    # Sort true labels by predicted scores\n",
    "    sorted_indices = np.argsort(y_scores)[::-1]\n",
    "    y_true_sorted = y_true[sorted_indices]\n",
    "    \n",
    "    # Calculate true positive rate (TPR) and false positive rate (FPR)\n",
    "    tpr = np.cumsum(y_true_sorted) / np.sum(y_true_sorted)  # True positive rate\n",
    "    fpr = np.cumsum(1 - y_true_sorted) / np.sum(1 - y_true_sorted)  # False positive rate\n",
    "    \n",
    "    # Compute AUC using the trapezoidal rule\n",
    "    auc = np.trapz(tpr, fpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation function\n",
    "def cross_validate(X, y, knn, n_splits=5):\n",
    "    # TODO: Implement cross-validation\n",
    "    # Compute ROC AUC scores\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X, y = X[indices], y[indices]\n",
    "    \n",
    "    fold_size = len(X) // n_splits\n",
    "    metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': [], 'roc_auc': []}\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        # Create train and validation sets\n",
    "        X_val = X[i * fold_size: (i + 1) * fold_size]\n",
    "        y_val = y[i * fold_size: (i + 1) * fold_size]\n",
    "        X_train = np.concatenate((X[:i * fold_size], X[(i + 1) * fold_size:]), axis=0)\n",
    "        y_train = np.concatenate((y[:i * fold_size], y[(i + 1) * fold_size:]), axis=0)\n",
    "        \n",
    "        # Fit the KNN model\n",
    "        knn.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict class labels for validation set\n",
    "        y_pred = knn.predict(X_val)\n",
    "        \n",
    "        # Compute performance metrics\n",
    "        metrics['accuracy'].append(accuracy(y_val, y_pred))\n",
    "        metrics['precision'].append(precision(y_val, y_pred))\n",
    "        metrics['recall'].append(recall(y_val, y_pred))\n",
    "        metrics['f1_score'].append(f1_score(y_val, y_pred))\n",
    "        metrics['roc_auc'].append(roc_auc(y_val, y_pred))  # ROC AUC from binary labels\n",
    "\n",
    "    # Return the average of each metric across all folds\n",
    "    return {key: np.mean(value) for key, value in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiazh\\AppData\\Local\\Temp\\ipykernel_23992\\1785421963.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data[feature].fillna(train_data[feature].mean(), inplace=True)\n",
      "C:\\Users\\tiazh\\AppData\\Local\\Temp\\ipykernel_23992\\1785421963.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_data[feature].fillna(train_data[feature].mean(), inplace=True)  # Use train mean for test\n",
      "C:\\Users\\tiazh\\AppData\\Local\\Temp\\ipykernel_23992\\3929796838.py:35: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(tpr, fpr)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "X, y, X_test = preprocess_data('train.csv', 'test.csv')\n",
    "\n",
    "# Create and evaluate model\n",
    "knn = KNN(k=5, distance_metric='euclidean')\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_validate(X, y, knn)\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# TODO: hyperparamters tuning\n",
    "best_k = None\n",
    "best_score = 0\n",
    "for k in range(1, 21):  # Try k from 1 to 20\n",
    "    knn = KNN(k=k, distance_metric='euclidean')\n",
    "    cv_scores = cross_validate(X, y, knn)\n",
    "    \n",
    "    if cv_scores['accuracy'] > best_score:\n",
    "        best_score = cv_scores['accuracy']\n",
    "        best_k = k\n",
    "\n",
    "print(f\"Best k: {best_k} with Accuracy: {best_score}\")\n",
    "\n",
    "\n",
    "# TODO: Train on full dataset with optimal hyperparameters and make predictions on test set\n",
    "knn = KNN(k=best_k, distance_metric='euclidean')\n",
    "knn.fit(X, y)\n",
    "test_predictions = knn.predict(X_test)\n",
    "\n",
    "# Save test predictions\n",
    "pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
